{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic differentiation with torch.autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "\n",
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x000001E043553A30>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x000001E0435538E0>\n"
     ]
    }
   ],
   "source": [
    "print('Gradient function for z =',z.grad_fn)\n",
    "print('Gradient function for loss =', loss.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1059, 0.1769, 0.2907],\n",
      "        [0.1059, 0.1769, 0.2907],\n",
      "        [0.1059, 0.1769, 0.2907],\n",
      "        [0.1059, 0.1769, 0.2907],\n",
      "        [0.1059, 0.1769, 0.2907]])\n",
      "tensor([0.1059, 0.1769, 0.2907])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):        \n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.300314  [    0/60000]\n",
      "loss: 2.296514  [ 6400/60000]\n",
      "loss: 2.284238  [12800/60000]\n",
      "loss: 2.278248  [19200/60000]\n",
      "loss: 2.265664  [25600/60000]\n",
      "loss: 2.245462  [32000/60000]\n",
      "loss: 2.248735  [38400/60000]\n",
      "loss: 2.235715  [44800/60000]\n",
      "loss: 2.240423  [51200/60000]\n",
      "loss: 2.195017  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.7%, Avg loss: 0.034677 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.219741  [    0/60000]\n",
      "loss: 2.208793  [ 6400/60000]\n",
      "loss: 2.189236  [12800/60000]\n",
      "loss: 2.196109  [19200/60000]\n",
      "loss: 2.141999  [25600/60000]\n",
      "loss: 2.115577  [32000/60000]\n",
      "loss: 2.133149  [38400/60000]\n",
      "loss: 2.105064  [44800/60000]\n",
      "loss: 2.130375  [51200/60000]\n",
      "loss: 2.018081  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 41.3%, Avg loss: 0.032347 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.092307  [    0/60000]\n",
      "loss: 2.066211  [ 6400/60000]\n",
      "loss: 2.029883  [12800/60000]\n",
      "loss: 2.055547  [19200/60000]\n",
      "loss: 1.928946  [25600/60000]\n",
      "loss: 1.907717  [32000/60000]\n",
      "loss: 1.940260  [38400/60000]\n",
      "loss: 1.898430  [44800/60000]\n",
      "loss: 1.967810  [51200/60000]\n",
      "loss: 1.771159  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 45.6%, Avg loss: 0.029053 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.912867  [    0/60000]\n",
      "loss: 1.867499  [ 6400/60000]\n",
      "loss: 1.826871  [12800/60000]\n",
      "loss: 1.875565  [19200/60000]\n",
      "loss: 1.671129  [25600/60000]\n",
      "loss: 1.699412  [32000/60000]\n",
      "loss: 1.726913  [38400/60000]\n",
      "loss: 1.702233  [44800/60000]\n",
      "loss: 1.792050  [51200/60000]\n",
      "loss: 1.551270  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 49.2%, Avg loss: 0.025923 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.729223  [    0/60000]\n",
      "loss: 1.683306  [ 6400/60000]\n",
      "loss: 1.640347  [12800/60000]\n",
      "loss: 1.711413  [19200/60000]\n",
      "loss: 1.450867  [25600/60000]\n",
      "loss: 1.531661  [32000/60000]\n",
      "loss: 1.551695  [38400/60000]\n",
      "loss: 1.547847  [44800/60000]\n",
      "loss: 1.636057  [51200/60000]\n",
      "loss: 1.390119  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 0.023455 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.572154  [    0/60000]\n",
      "loss: 1.540112  [ 6400/60000]\n",
      "loss: 1.488317  [12800/60000]\n",
      "loss: 1.589282  [19200/60000]\n",
      "loss: 1.294861  [25600/60000]\n",
      "loss: 1.404892  [32000/60000]\n",
      "loss: 1.429555  [38400/60000]\n",
      "loss: 1.440727  [44800/60000]\n",
      "loss: 1.522492  [51200/60000]\n",
      "loss: 1.283205  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 50.6%, Avg loss: 0.021748 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.457726  [    0/60000]\n",
      "loss: 1.440935  [ 6400/60000]\n",
      "loss: 1.378239  [12800/60000]\n",
      "loss: 1.504780  [19200/60000]\n",
      "loss: 1.195579  [25600/60000]\n",
      "loss: 1.312910  [32000/60000]\n",
      "loss: 1.348296  [38400/60000]\n",
      "loss: 1.367326  [44800/60000]\n",
      "loss: 1.439699  [51200/60000]\n",
      "loss: 1.214334  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 51.9%, Avg loss: 0.020584 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.375085  [    0/60000]\n",
      "loss: 1.372806  [ 6400/60000]\n",
      "loss: 1.298289  [12800/60000]\n",
      "loss: 1.448279  [19200/60000]\n",
      "loss: 1.132216  [25600/60000]\n",
      "loss: 1.246543  [32000/60000]\n",
      "loss: 1.294989  [38400/60000]\n",
      "loss: 1.317400  [44800/60000]\n",
      "loss: 1.380628  [51200/60000]\n",
      "loss: 1.169037  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.019784 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.313670  [    0/60000]\n",
      "loss: 1.326085  [ 6400/60000]\n",
      "loss: 1.238474  [12800/60000]\n",
      "loss: 1.408741  [19200/60000]\n",
      "loss: 1.089431  [25600/60000]\n",
      "loss: 1.196784  [32000/60000]\n",
      "loss: 1.257384  [38400/60000]\n",
      "loss: 1.282634  [44800/60000]\n",
      "loss: 1.335132  [51200/60000]\n",
      "loss: 1.135304  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 54.0%, Avg loss: 0.019195 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.265034  [    0/60000]\n",
      "loss: 1.291167  [ 6400/60000]\n",
      "loss: 1.191358  [12800/60000]\n",
      "loss: 1.377041  [19200/60000]\n",
      "loss: 1.052366  [25600/60000]\n",
      "loss: 1.087801  [32000/60000]\n",
      "loss: 1.185201  [38400/60000]\n",
      "loss: 1.148115  [44800/60000]\n",
      "loss: 1.116327  [51200/60000]\n",
      "loss: 1.108210  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 0.017359 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"data/model.pth\")\n",
    "\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
